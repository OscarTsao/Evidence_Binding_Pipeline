# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a sentence-criterion (S-C) evidence retrieval pipeline for mental health research. Given a Reddit post and a DSM-5 criterion, the system retrieves evidence sentences supporting the criterion for Major Depressive Disorder (MDD) assessment.

### Best Model Configuration (HPO-Optimized)
- **Retriever:** NV-Embed-v2 (nvidia/NV-Embed-v2) - Best from 25+ retrievers
- **Reranker:** Jina-Reranker-v3 (jinaai/jina-reranker-v3) - Best from 15+ rerankers
- **Performance:** nDCG@10 = 0.8658 (from 324 model combinations tested)

### Pipeline Options
1. **Zoo Pipeline (RECOMMENDED)** - Uses best HPO model combo (nv-embed-v2 + jina-reranker-v3)
2. **Legacy Pipeline** - Uses BGE-M3 + Jina-v3 (for backward compatibility)

## Commands

### Setup
```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
```

### Required Data Layout
Place data locally (not tracked in git):
```
data/
  redsm5/
    redsm5_posts.csv       # Columns: post_id, text
    redsm5_annotations.csv # Evidence annotations
  DSM5/
    MDD_Criteira.json      # DSM-5 MDD criteria definitions
  groundtruth/             # Generated by build scripts
    evidence_sentence_groundtruth.csv
    sentence_corpus.jsonl
  cache/                   # Auto-generated embedding caches
```

### Build Data Artifacts
```bash
# Build groundtruth labels
python scripts/build_groundtruth.py --data_dir data --output data/groundtruth/evidence_sentence_groundtruth.csv

# Build sentence corpus
python scripts/build_sentence_corpus.py --data_dir data --output data/groundtruth/sentence_corpus.jsonl
```

### Evaluation (RECOMMENDED - Zoo Pipeline)
```bash
# Evaluate with best HPO model combo (nv-embed-v2 + jina-reranker-v3)
python scripts/eval_zoo_pipeline.py --config configs/default.yaml --split test

# Single query inference with best models
python scripts/run_single_zoo.py --config configs/default.yaml --post_id <POST_ID> --criterion_id <CRITERION_ID>
```

### Evaluation (Legacy Pipeline)
```bash
# Evaluate with BGE-M3 (legacy)
python scripts/eval_sc_pipeline.py --config configs/default.yaml --output outputs/eval_summary.json

# Single query inference (legacy)
python scripts/run_single.py --config configs/default.yaml --post_id <POST_ID> --criterion_id <CRITERION_ID>
```

### Hyperparameter Optimization
```bash
# Run HPO for all retriever x reranker combinations
python scripts/run_inference_hpo_all_combos.py --all_zoo --n_trials 50

# Single combo HPO
python scripts/hpo_inference.py --config configs/hpo_inference.yaml --n_trials 200 --study_name sc_inference

# Multi-GPU HPO (one worker per GPU)
python scripts/launch_hpo_multi_gpu.py --config configs/hpo_inference.yaml --study_name sc_inference --n_trials_per_worker 200

# Export best config after HPO
python scripts/export_best_config.py --study_name sc_inference --storage sqlite:///outputs/hpo/optuna.db
```

### Testing
```bash
pytest                        # Run all tests
pytest tests/test_metrics.py  # Run single test file
pytest -k "test_name"         # Run tests matching pattern
pytest -v                     # Verbose output
```

## Architecture

### Zoo Pipeline (`src/final_sc_review/pipeline/zoo_pipeline.py`)
The recommended pipeline using retriever and reranker zoos:
1. **Stage 1 - Retrieval:** Uses any retriever from zoo (default: nv-embed-v2)
2. **Stage 2 - Reranking:** Uses any reranker from zoo (default: jina-reranker-v3)

Key classes: `ZooPipeline`, `ZooPipelineConfig`

### Three-Stage Pipeline (`src/final_sc_review/pipeline/three_stage.py`)
Legacy pipeline for backward compatibility:
1. **Stage 1 - Hybrid Retrieval:** BGE-M3 encodes sentences with dense, sparse, and ColBERT representations
2. **Stage 2 - Score Fusion:** Dense, sparse, and ColBERT scores combined via weighted sum or RRF
3. **Stage 3 - Reranking:** Top-k candidates reranked by Jina-v3

Key classes: `ThreeStagePipeline`, `PipelineConfig`

### Retriever Zoo (`src/final_sc_review/retriever/zoo.py`)
Unified interface for 25+ retriever models with caching:
- `RetrieverZoo` - Factory for retrievers
- `BaseRetriever` - Abstract base with `retrieve_within_post()` method
- Implementations: `DenseRetriever`, `BM25Retriever`, `BGEM3ZooRetriever`, `NVEmbedRetriever`, `ColBERTv2Retriever`, `SPLADERetriever`
- Models include: nv-embed-v2, qwen3-embed-*, gte-*, bge-*, e5-*, stella-*, arctic-*, llama-embed-8b

### Reranker Zoo (`src/final_sc_review/reranker/zoo.py`)
Unified interface for 15+ reranker models:
- `RerankerZoo` - Factory for rerankers
- `BaseReranker` - Abstract base with `rerank()` and `rerank_batch()` methods
- Implementations: `CrossEncoderReranker`, `ListwiseReranker`, `BGELightweightReranker`
- Models include: jina-reranker-v3, mxbai-rerank-*, qwen3-reranker-*, bge-reranker-*

### Key Modules
- `src/final_sc_review/data/schemas.py` - Core dataclasses: `Post`, `Criterion`, `Sentence`, `GroundTruthRow`
- `src/final_sc_review/data/io.py` - Data loading: `load_posts()`, `load_criteria()`, `load_groundtruth()`, `load_sentence_corpus()`
- `src/final_sc_review/data/splits.py` - Post-ID-disjoint splits: `split_post_ids()`, `k_fold_post_ids()`
- `src/final_sc_review/metrics/ranking.py` - Metrics: `recall_at_k()`, `mrr_at_k()`, `map_at_k()`, `ndcg_at_k()`
- `src/final_sc_review/hpo/` - Optuna-based HPO with cache-first strategy

### GNN Enhancements (Research)
- `src/final_sc_review/gnn/` - Graph Neural Network models for:
  - P1: NE Gate (`models/p1_ne_gate.py`) - No-evidence detection
  - P2: Dynamic-K (`models/p2_dynamic_k.py`) - Adaptive top-k selection
  - P3: Graph Reranker (`models/p3_graph_reranker.py`) - GNN-based reranking
  - P4: Criterion-Aware GNN (`models/p4_hetero.py`) - Heterogeneous graph (AUROC=0.8967)
- Supporting modules: `graphs/builder.py`, `graphs/features.py`, `training/trainer.py`, `evaluation/cv.py`

### Clinical Deployment (`src/final_sc_review/clinical/`)
- High-recall clinical deployment mode with configurable thresholds
- Dynamic-K selection and three-state gate (positive/negative/uncertain)

### LLM Integration (`src/final_sc_review/llm/`)
- Hybrid pipeline with LLM verification
- Gemini client for external validation
- A10 classifier for criterion-specific evidence detection

### Data Flow
- **Input:** Posts (`redsm5_posts.csv`) + Criteria (`MDD_Criteira.json`) + Annotations (`redsm5_annotations.csv`)
- **Groundtruth:** `evidence_sentence_groundtruth.csv` with columns: `post_id, criterion, sid, sent_uid, sentence, groundtruth`
- **Sentence corpus:** `sentence_corpus.jsonl` with canonical sentence splitting and `sent_uid = f"{post_id}_{sid}"`

### Key Constraints
- All splits are **post_id-disjoint** (no post appears in multiple splits)
- HPO uses DEV split only; TEST is evaluated once with the best config
- Retrieval is always **within-post** (candidate pool = sentences from the queried post)
- Cache fingerprint includes corpus hash + model config; auto-rebuilds on mismatch

## Configuration

YAML configs control all pipeline parameters. Key sections:
- `paths` - Data directories, groundtruth/corpus paths, cache location
- `models` - Model names (both zoo and legacy keys supported)
- `retriever` - Fusion weights, top-k values, normalization method
- `split` - Train/val/test ratios and seed
- `evaluation` - Metrics at K values, which split to evaluate

### Best HPO Parameters
```yaml
retriever:
  top_k_retriever: 24
  top_k_final: 10
  use_sparse: false
  use_colbert: false
  fusion_method: rrf
  score_normalization: none
  rrf_k: 60
```

### Config Keys
- **Zoo Pipeline:** `models.retriever_name`, `models.reranker_name`
- **Legacy Pipeline:** `models.bge_m3`, `models.jina_v3`

The `top_k_retriever` controls retrieval pool size, `top_k_rerank` controls reranker input size, and `top_k_final` controls output size.

## HPO Results Summary

| Rank | Retriever | Reranker | nDCG@10 |
|------|-----------|----------|---------|
| 1 | nv-embed-v2 | jina-reranker-v3 | 0.8658 |
| 2 | qwen3-embed-4b | jina-reranker-v3 | 0.8534 |
| 3 | llama-embed-8b | jina-reranker-v3 | 0.8489 |

Full results: `outputs/hpo_inference_combos/full_results.csv`

## Test Suite

Key test categories that enforce research integrity:
- `test_no_leakage_splits.py` - Verifies post_id-disjoint splits
- `test_candidate_pool_within_post_only.py` - Ensures retrieval stays within-post
- `test_hpo_never_uses_test_split.py` - Guarantees HPO doesn't touch test set
- `test_cache_fingerprint.py` - Validates cache invalidation on config changes
- `test_gnn_no_leakage.py`, `test_no_leaky_features.py` - GNN feature leakage prevention
